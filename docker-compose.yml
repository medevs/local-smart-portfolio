# =============================================================================
# Docker Compose - AI Portfolio Application
# =============================================================================
# Development and Production-ready configuration
# =============================================================================

services:
  # ===========================================================================
  # Backend Service (FastAPI)
  # ===========================================================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: portfolio-backend
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # Application Settings
      - APP_NAME=${APP_NAME:-AI Portfolio Backend}
      - APP_VERSION=${APP_VERSION:-1.0.0}
      - DEBUG=${DEBUG:-false}

      # Server Settings
      - HOST=0.0.0.0
      - PORT=8000

      # CORS Settings (comma-separated)
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:3000,http://localhost:3001}

      # Ollama LLM Settings
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:3b}

      # ChromaDB Settings
      - CHROMA_PERSIST_DIR=/app/data/chroma_db
      - CHROMA_COLLECTION_NAME=${CHROMA_COLLECTION_NAME:-portfolio_docs}

      # Document Processing
      - UPLOAD_DIR=/app/data/documents
      - MAX_FILE_SIZE_MB=${MAX_FILE_SIZE_MB:-10}
      - ALLOWED_EXTENSIONS=${ALLOWED_EXTENSIONS:-.pdf,.md,.txt,.docx}

      # RAG Settings
      - CHUNK_SIZE=${CHUNK_SIZE:-500}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-50}
      - TOP_K_RESULTS=${TOP_K_RESULTS:-3}

      # Embedding Model (Ollama embedding model)
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}

      # Security (REQUIRED - set in .env file)
      - ADMIN_API_KEY=${ADMIN_API_KEY}
    volumes:
      # Persist ChromaDB data
      - chroma_data:/app/data/chroma_db
      # Persist uploaded documents
      - documents_data:/app/data/documents
      # Persist logs
      - backend_logs:/app/logs
    networks:
      - portfolio-network
    depends_on:
      ollama-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ===========================================================================
  # Frontend Service (Next.js)
  # ===========================================================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=http://localhost:8000
    container_name: portfolio-frontend
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      # Admin API Key (for frontend admin operations)
      - NEXT_PUBLIC_ADMIN_API_KEY=${ADMIN_API_KEY}
    networks:
      - portfolio-network
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ===========================================================================
  # Ollama Service (Local LLM)
  # ===========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: portfolio-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      # Persist Ollama models
      - ollama_data:/root/.ollama
    networks:
      - portfolio-network
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    # Optional: Set resource limits
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

  # ===========================================================================
  # Ollama Model Initialization (Pulls required models)
  # ===========================================================================
  # This init container ensures all required Ollama models are pulled before
  # the backend starts. It runs once and exits successfully.
  # ===========================================================================
  ollama-init:
    image: curlimages/curl:latest
    container_name: portfolio-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - portfolio-network
    environment:
      - OLLAMA_HOST=ollama:11434
      - LLM_MODEL=${OLLAMA_MODEL:-llama3.2:3b}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "=== Ollama Model Initialization ==="
        echo "Waiting for Ollama to be ready..."

        # Wait for Ollama API
        until curl -sf http://$$OLLAMA_HOST/api/tags > /dev/null 2>&1; do
          echo "Waiting for Ollama..."
          sleep 2
        done
        echo "Ollama is ready!"

        # Function to pull model if not exists
        pull_model() {
          MODEL=$$1
          echo "Checking model: $$MODEL"

          # Check if model exists
          if curl -sf http://$$OLLAMA_HOST/api/tags | grep -q "\"name\":\"$$MODEL\""; then
            echo "Model $$MODEL already exists, skipping pull"
          else
            echo "Pulling model: $$MODEL (this may take a while)..."
            curl -X POST http://$$OLLAMA_HOST/api/pull -d "{\"name\": \"$$MODEL\"}" --no-buffer
            echo ""
            echo "Model $$MODEL pulled successfully!"
          fi
        }

        # Pull LLM model
        pull_model "$$LLM_MODEL"

        # Pull embedding model
        pull_model "$$EMBEDDING_MODEL"

        echo "=== All models ready! ==="
    restart: "no"

# =============================================================================
# Volumes (Data Persistence)
# =============================================================================
volumes:
  chroma_data:
    driver: local
  documents_data:
    driver: local
  backend_logs:
    driver: local
  ollama_data:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  portfolio-network:
    driver: bridge
